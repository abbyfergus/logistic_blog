{"title":"Logistic Regression Tutorial","markdown":{"yaml":{"title":"Logistic Regression Tutorial","author":"Abby Fergus","date":"2024-05-08","categories":["news","code","analysis"],"image":"log.jpg"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nNot sure about logistic regression? Do not fear! Read here!\n\nYou might be thinking \"ugh\" I hate statistics what is logistic regression anyways?\n\n![](images/1-01.png){width=\"399\"}\n\nBut I promise you it's not only a very helpful method but it is quite simple to implement! By the end of this post you'll be sleeping soundly knowing you mastered the logistic regression.\n\n![](images/2.png){width=\"385\"}\n\n## Why do we need logistic regression?\n\nOkay let's get started! Why do we even need logistic regression? When should you use it?\n\nSometimes linear regression isn't the best fit for our data. This can be the case for a variety of reasons. One common case where this might happen is when our response variable is binary, or has two distinct levels. For instance if we have bi-modal data as in the image below, it is clear that although the linear model is capturing the fact that as our x variable increases, the y variable increases, it misses out on the real pattern of the data. A linear model assumes that as x increases, y increases at a fixed rate. In our data, however, it is clear that this is not the case. In fact, the y variable is constant until a certain threshold is reached by the x variable.\n\n![](images/clipboard-1267359291.png){width=\"349\"}\n\nIn contrast, if we fit a logistic model instead, you can see that it does a much better job of describing the data! As mentioned earlier, logistic regression is recommended for any binary data (and spoiler alert it is often extended to capture categorical data of many shapes and sizes). For our purposes, however, let's stick with purely binary data. This could be yes/no responses, accuracy scored as 0 or 1, heads/tails, and the list goes on.\n\n![](images/clipboard-3142948472.png){width=\"356\"}\n\n## What even is logistic regression?\n\nNow you're probably thinking, okay great it seems like logistic regression is helpful for categorical variables but what is it? The equation is similar to a linear model but instead of predicting the y value, we are predicting the \"log odds.\" The log odds is the logarithm of the odds of an event happening where $p$ is the probability of the event happening and 1-$p$ is the probability that the event won't happen.\n\n$$\\begin{equation*}\n\\log\\left(\\frac{p}{1 - p}\\right)=b_0+b_1X \n \\end{equation*}$$\n\nIf we take a yes/no example we can think of $p$ as being the probability that the response will be a yes and 1-$p$ as the probability that the response will be a no.\n\n## How can I implement logistic regression in my work?\n\nImplementing logistic regression is a simple, easy extension of linear regression. First load the packages that you will need.\n\n```{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}\n# load packages\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom.mixed)\n```\n\nLet's use an example data set: iris. This data set includes information about three species of irises including their sepal length and width and their petal length and width. If we look at just two of the species, this provides a great example of a binary variable. Let's see if we can build a logistic regression to predict species by sepal length. First load and format the data.\n\n```{r}\n# load data \ndata <- as.data.frame(iris)\n\n# format data\ndata <- data %>%\n  filter(Species != \"virginica\") %>% # get rid of virignica so we only have 2\n  mutate(Species= # make species numeric\n           case_when(\n             Species == \"versicolor\" ~ 1,\n             Species == \"setosa\" ~ 0))\n```\n\nNow we are ready to perform our logistic regression. The format is the same as a linear model except we will include an additional argument \"family = binomial\" to indicate that the data is binary. Note: if you want to run a mixed effects model you would use the function glmer instead.\n\n```{r}\nlog <- glm(Species ~ Sepal.Length, family = binomial, data = data)\n\nlog  %>% \n  tidy(conf.int = TRUE) %>%\n  kable()\n```\n\nGreat! As you can see in the output above, sepal length significantly, positively predicts species ($p$ \\< .001). However, interpretation of the $b$ values is slightly different from linear regression. Now let's think back to our logistic regression formula. Remember the left side of the equation is what we call the log odds so the intercept in this case is the log odds when X is equal to 0 (when the sepal length is 0). In our case this is nonsensical but in other scenarios it might make sense where a 0 value is interpretable. This would give you the baseline log odds. Now to interpret $b1$, this is the change in the log odds of the y variable going from 0 to 1 for a one unit increase in X. For our case, this means for a one cm increase in sepal length, the log odds of the species being versicolor increases by 5.14.\n\nBut what are log odds anyways? You might be thinking okay great but that is all a bunch of jargon to me. There's a few things we can do to make it more interpretable.\n\n### Odds ratio\n\nThe first thing we can do is get the odds ratio instead of the log odds. Recall, the odds ratio is just the part within the log() of our logistic regression formula: $$\\begin{equation*}\n\\left(\\frac{p}{1 - p}\\right) \n \\end{equation*}$$\n\nWhich is defined as the probability that an event will happen $p$ over the probability that an event won't happen (1 - $p$). So to get the odds ratio, all we have to do is exponentiate our result. Then, we know that if the result is greater than 1, the event is more likely to occur (in our case more likely to be versicolor) and if the result is less than 1, the event is less likely to occur (more likely to be setosa). This can be done simply as shown below.\n\n```{r}\ntidy(log, exponentiate = TRUE, conf.int = TRUE) %>%\n  kable()\n```\n\nNow looking at our coefficients, we can see that the odds ratio is greater than 1 showing that as sepal length increases, the species is more likely to be versicolor. More specifically, the odds of the species being versicolor is 171 times the odds of the species being setosa for a 1 cm increase in sepal length. We can also convert this to a percentage as shown below.\n\n```{r}\n# convert to percentage \n(171-1) * 100\n```\n\nWow! The odds of the species being versicolor is 17000% higher than setosa for a 1 cm increase in sepal length.\n\n### Probability \n\nWe can even get a probability of the event happening or not happening for a specific X value (in our case the probability of the species being versicolor for a specific sepal length. Luckily a simple function in r can do this for us. All we need to do is use the plogis function in plug in the following information: plogis($b0$ + $b1$\\*sepal.length). In our case, let's predict the probability for a sepal length of 4.6 cm.\n\n```{r}\n# convert to probability \nplogis(-27.83 + (5.14)*4.6)\n```\n\nThe probability of an iris being versicolor with a sepal length of 4.6 cm is very low at about 1.5%. This makes sense because if we look at our original data, irises with sepal lengths of 4.6 cm were always categorized as setosa.\n\n## Conclusion\n\nNow you've learned the basics of logisitic regression! It really is very simple to implement and has extensions in all sorts of other areas of statistics. Just remember, when you are dealing with binary data, always use logistic!\n\n![](images/Binary data.png){width=\"362\"}\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nNot sure about logistic regression? Do not fear! Read here!\n\nYou might be thinking \"ugh\" I hate statistics what is logistic regression anyways?\n\n![](images/1-01.png){width=\"399\"}\n\nBut I promise you it's not only a very helpful method but it is quite simple to implement! By the end of this post you'll be sleeping soundly knowing you mastered the logistic regression.\n\n![](images/2.png){width=\"385\"}\n\n## Why do we need logistic regression?\n\nOkay let's get started! Why do we even need logistic regression? When should you use it?\n\nSometimes linear regression isn't the best fit for our data. This can be the case for a variety of reasons. One common case where this might happen is when our response variable is binary, or has two distinct levels. For instance if we have bi-modal data as in the image below, it is clear that although the linear model is capturing the fact that as our x variable increases, the y variable increases, it misses out on the real pattern of the data. A linear model assumes that as x increases, y increases at a fixed rate. In our data, however, it is clear that this is not the case. In fact, the y variable is constant until a certain threshold is reached by the x variable.\n\n![](images/clipboard-1267359291.png){width=\"349\"}\n\nIn contrast, if we fit a logistic model instead, you can see that it does a much better job of describing the data! As mentioned earlier, logistic regression is recommended for any binary data (and spoiler alert it is often extended to capture categorical data of many shapes and sizes). For our purposes, however, let's stick with purely binary data. This could be yes/no responses, accuracy scored as 0 or 1, heads/tails, and the list goes on.\n\n![](images/clipboard-3142948472.png){width=\"356\"}\n\n## What even is logistic regression?\n\nNow you're probably thinking, okay great it seems like logistic regression is helpful for categorical variables but what is it? The equation is similar to a linear model but instead of predicting the y value, we are predicting the \"log odds.\" The log odds is the logarithm of the odds of an event happening where $p$ is the probability of the event happening and 1-$p$ is the probability that the event won't happen.\n\n$$\\begin{equation*}\n\\log\\left(\\frac{p}{1 - p}\\right)=b_0+b_1X \n \\end{equation*}$$\n\nIf we take a yes/no example we can think of $p$ as being the probability that the response will be a yes and 1-$p$ as the probability that the response will be a no.\n\n## How can I implement logistic regression in my work?\n\nImplementing logistic regression is a simple, easy extension of linear regression. First load the packages that you will need.\n\n```{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}\n# load packages\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom.mixed)\n```\n\nLet's use an example data set: iris. This data set includes information about three species of irises including their sepal length and width and their petal length and width. If we look at just two of the species, this provides a great example of a binary variable. Let's see if we can build a logistic regression to predict species by sepal length. First load and format the data.\n\n```{r}\n# load data \ndata <- as.data.frame(iris)\n\n# format data\ndata <- data %>%\n  filter(Species != \"virginica\") %>% # get rid of virignica so we only have 2\n  mutate(Species= # make species numeric\n           case_when(\n             Species == \"versicolor\" ~ 1,\n             Species == \"setosa\" ~ 0))\n```\n\nNow we are ready to perform our logistic regression. The format is the same as a linear model except we will include an additional argument \"family = binomial\" to indicate that the data is binary. Note: if you want to run a mixed effects model you would use the function glmer instead.\n\n```{r}\nlog <- glm(Species ~ Sepal.Length, family = binomial, data = data)\n\nlog  %>% \n  tidy(conf.int = TRUE) %>%\n  kable()\n```\n\nGreat! As you can see in the output above, sepal length significantly, positively predicts species ($p$ \\< .001). However, interpretation of the $b$ values is slightly different from linear regression. Now let's think back to our logistic regression formula. Remember the left side of the equation is what we call the log odds so the intercept in this case is the log odds when X is equal to 0 (when the sepal length is 0). In our case this is nonsensical but in other scenarios it might make sense where a 0 value is interpretable. This would give you the baseline log odds. Now to interpret $b1$, this is the change in the log odds of the y variable going from 0 to 1 for a one unit increase in X. For our case, this means for a one cm increase in sepal length, the log odds of the species being versicolor increases by 5.14.\n\nBut what are log odds anyways? You might be thinking okay great but that is all a bunch of jargon to me. There's a few things we can do to make it more interpretable.\n\n### Odds ratio\n\nThe first thing we can do is get the odds ratio instead of the log odds. Recall, the odds ratio is just the part within the log() of our logistic regression formula: $$\\begin{equation*}\n\\left(\\frac{p}{1 - p}\\right) \n \\end{equation*}$$\n\nWhich is defined as the probability that an event will happen $p$ over the probability that an event won't happen (1 - $p$). So to get the odds ratio, all we have to do is exponentiate our result. Then, we know that if the result is greater than 1, the event is more likely to occur (in our case more likely to be versicolor) and if the result is less than 1, the event is less likely to occur (more likely to be setosa). This can be done simply as shown below.\n\n```{r}\ntidy(log, exponentiate = TRUE, conf.int = TRUE) %>%\n  kable()\n```\n\nNow looking at our coefficients, we can see that the odds ratio is greater than 1 showing that as sepal length increases, the species is more likely to be versicolor. More specifically, the odds of the species being versicolor is 171 times the odds of the species being setosa for a 1 cm increase in sepal length. We can also convert this to a percentage as shown below.\n\n```{r}\n# convert to percentage \n(171-1) * 100\n```\n\nWow! The odds of the species being versicolor is 17000% higher than setosa for a 1 cm increase in sepal length.\n\n### Probability \n\nWe can even get a probability of the event happening or not happening for a specific X value (in our case the probability of the species being versicolor for a specific sepal length. Luckily a simple function in r can do this for us. All we need to do is use the plogis function in plug in the following information: plogis($b0$ + $b1$\\*sepal.length). In our case, let's predict the probability for a sepal length of 4.6 cm.\n\n```{r}\n# convert to probability \nplogis(-27.83 + (5.14)*4.6)\n```\n\nThe probability of an iris being versicolor with a sepal length of 4.6 cm is very low at about 1.5%. This makes sense because if we look at our original data, irises with sepal lengths of 4.6 cm were always categorized as setosa.\n\n## Conclusion\n\nNow you've learned the basics of logisitic regression! It really is very simple to implement and has extensions in all sorts of other areas of statistics. Just remember, when you are dealing with binary data, always use logistic!\n\n![](images/Binary data.png){width=\"362\"}\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Logistic Regression Tutorial","author":"Abby Fergus","date":"2024-05-08","categories":["news","code","analysis"],"image":"log.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}